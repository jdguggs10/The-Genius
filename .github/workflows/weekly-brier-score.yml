name: Weekly Brier Score Pipeline

on:
  schedule:
    # Run every Sunday at 6 AM UTC
    - cron: '0 6 * * 0'
  workflow_dispatch:
    inputs:
      days_back:
        description: 'Number of days to analyze'
        required: false
        default: '7'
        type: string
      dry_run:
        description: 'Run in dry-run mode'
        required: false
        default: true
        type: boolean

jobs:
  brier-score-analysis:
    runs-on: ubuntu-latest
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt
    
    - name: Run Weekly Brier Score Pipeline
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        cd backend
        python scripts/weekly_brier_score_pipeline.py \
          --days ${{ github.event.inputs.days_back || '7' }} \
          --output brier_report_$(date +%Y%m%d).json \
          --verbose
    
    - name: Upload Brier Score Report
      uses: actions/upload-artifact@v3
      with:
        name: brier-score-report-${{ github.run_number }}
        path: backend/brier_report_*.json
        retention-days: 30
    
    - name: Run Auto-tune Analysis
      if: ${{ !github.event.inputs.dry_run }}
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        cd backend
        python -c "
        import sys
        sys.path.append('.')
        from app.services.confidence_phrase_tuner import confidence_phrase_tuner
        import json
        
        result = confidence_phrase_tuner.auto_tune_phrases(days_back=7, dry_run=False)
        print('Auto-tune result:', json.dumps(result, indent=2, default=str))
        
        if result.get('bands_adjusted', 0) > 0:
            print(f'âœ… Auto-tuned {result[\"bands_adjusted\"]} confidence bands')
        else:
            print('â„¹ï¸ No confidence bands needed adjustment')
        "
    
    - name: Check Calibration Status
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        cd backend
        python -c "
        import sys
        sys.path.append('.')
        from app.services.confidence_scoring import confidence_scoring_service
        import json
        
        # Get recent Brier score
        brier_stats = confidence_scoring_service.calculate_brier_score(7)
        print('ðŸ“Š Brier Score Analysis:')
        print(json.dumps(brier_stats, indent=2, default=str))
        
        if brier_stats.get('needs_calibration', False):
            print('âš ï¸ ATTENTION: Confidence calibration needed!')
            print(f'Brier score {brier_stats[\"brier_score\"]:.4f} exceeds 0.25 threshold')
        else:
            print('âœ… Confidence calibration within acceptable range')
        "
    
    - name: Create Issue for High Brier Score
      if: ${{ !github.event.inputs.dry_run }}
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Check if Brier score report exists and read it
          const reportFiles = fs.readdirSync('backend').filter(f => f.startsWith('brier_report_'));
          if (reportFiles.length === 0) {
            console.log('No Brier score report found');
            return;
          }
          
          const reportPath = path.join('backend', reportFiles[0]);
          const report = JSON.parse(fs.readFileSync(reportPath, 'utf8'));
          
          if (report.summary && report.summary.needs_calibration) {
            const brierScore = report.summary.brier_score;
            const accuracy = report.summary.accuracy;
            const entriesAnalyzed = report.summary.entries_analyzed;
            
            const issueTitle = `ðŸŽ¯ Confidence Calibration Alert: Brier Score ${brierScore.toFixed(4)}`;
            const issueBody = `
          ## Confidence Calibration Issue Detected
          
          **Brier Score:** ${brierScore.toFixed(4)} (threshold: 0.25)
          **Accuracy:** ${(accuracy * 100).toFixed(1)}%
          **Entries Analyzed:** ${entriesAnalyzed}
          **Period:** Last 7 days
          
          ### Recommendations
          ${report.recommendations.map(rec => \`- **\${rec.priority.toUpperCase()}:** \${rec.description}\`).join('\\n')}
          
          ### Analysis Summary
          \`\`\`json
          ${JSON.stringify(report.summary, null, 2)}
          \`\`\`
          
          *This issue was automatically created by the Weekly Brier Score Pipeline*
          *Report generated: ${report.timestamp}*
          `;
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: issueTitle,
              body: issueBody,
              labels: ['confidence-calibration', 'automated', 'priority']
            });
            
            console.log('Created issue for high Brier score');
          } else {
            console.log('Brier score within acceptable range, no issue created');
          }

  notify-team:
    needs: brier-score-analysis
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Notify Team
      uses: actions/github-script@v6
      with:
        script: |
          const status = '${{ needs.brier-score-analysis.result }}';
          const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
          
          console.log(`ðŸ“Š Weekly Brier Score Pipeline completed with status: ${status}`);
          console.log(`ðŸ“‹ Full report available at: ${runUrl}`);
          
          // You can add Slack/email notifications here if needed 